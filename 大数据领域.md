# Hadoop
是一个大数据解决方案。它提供了一套分布式系统基础架构。 核心内容包含 HDFS 和 MapReduce。hdfs是提供数据存储的，mapreduce是方便数据计算的。hadoop2.0以后引入Yarn。

1. hdfs可以分为namenode和datanode。namenode负责保存元数据的基本信息，datanode直接存放数据本身；
2. mapreduce对应jobtracker和tasktracker. jobtracker负责分发任务，tasktracker负责执行具体任务；
3. 对应到master/slave架构，namenode和jobtracker就应该对应到master, datanode和tasktracker就应该对应到slave.

## HDFS
>client

Client（代表用 户） 通过与 NameNode 和 DataNode 交互访问 HDFS 中 的文件。 Client提供了一个文件系统接口供用户调用。

> NameNode

整个Hadoop 集群中只有一个 NameNode。 它是整个系统的“ 总管”， 负责管理 HDFS的目录树和相关的文件元数据信息。 这些信息是以“ fsimage”（ HDFS 元数据镜像文件）和“ editlog”（HDFS 文件改动日志）两个文件形式存放在本地磁盘，当 HDFS 重启时重新构造出来的。

此外， NameNode 还负责监控各个 DataNode 的健康状态， 一旦发现某个DataNode 挂掉，则将该 DataNode 移出 HDFS 并重新备份其上面的数据。

> Secondary NameNode

Secondary NameNode 最重要的任务并不是为 NameNode 元数据进行热备份， 而是定期合并 fsimage 和 edits 日志， 并传输给 NameNode。 这里需要注意的是，为了减小 NameNode压力， NameNode 自己并不会合并fsimage 和 edits， 并将文件存储到磁盘上， 而是交由Secondary NameNode 完成。

> dataNode

一般而言， 每个 Slave 节点上安装一个 DataNode， 它负责实际的数据存储， 并将数据信息定期汇报给 NameNode。 

DataNode 以固定大小的 block 为基本单位组织文件内容， 默认情况下 block 大小为 64MB。 当用户上传一个大的文件到 HDFS 上时， 该文件会被切分成若干个 block， 分别存储到不同的 DataNode ； 同时，为了保证数据可靠， 会将同一个block以流水线方式写到若干个（默认是 3，该参数可配置）不同的 DataNode 上。 这种文件切割后存储的过程是对用户透明的.

## MapReduce

同 HDFS 一样，Hadoop MapReduce 也采用了 Master/Slave（M/S）架构，具体如图所示。它主要由以下几个组件组成：Client、JobTracker、TaskTracker 和 Task。 


![在这里插入图片描述](https://img-blog.csdnimg.cn/97ff1f0373ac460d822e0e9f2150627a.png)

- Client: 用户编写的 MapReduce 程序通过 Client 提交到 JobTracker 端； 同时， 用户可通过 Client 提供的一些接口查看作业运行状态。 在 Hadoop 内部用“作业”（Job） 表示 MapReduce 程序。 一个MapReduce 程序可对应若干个作业，而每个作业会被分解成若干个 Map/Reduce 任务（Task）。
- JobTracker: JobTracker 主要负责资源监控和作业调度。JobTracker监控所有TaskTracker与作业的健康状况，一旦发现失败情况后，其会将相应的任务转移到其他节点；同时JobTracker 会跟踪任务的执行进度、资源使用量等信息，并将这些信息告诉任务调度器，而调度器会在资源出现空闲时，选择合适的任务使用这些资源。在 Hadoop 中，任务调度器是一个可插拔的模块，用户可以根据自己的需要设计相应的调度器。
- TaskTracker: TaskTracker 会周期性地通过 Heartbeat 将本节点上资源的使用情况和任务的运行进度汇报给 JobTracker， 同时接收 JobTracker 发送过来的命令并执行相应的操作（如启动新任务、 杀死任务等）。TaskTracker 使用“slot” 等量划分本节点上的资源量。“slot” 代表计算资源（CPU、内存等）。一个Task 获取到一个slot 后才有机会运行，而Hadoop 调度器的作用就是将各个TaskTracker 上的空闲 slot 分配给 Task 使用。 slot 分为 Map slot 和 Reduce slot 两种，分别供 MapTask 和 Reduce Task 使用。 TaskTracker 通过 slot 数目（可配置参数）限定 Task 的并发度。
- Task: Task 分为 Map Task 和 Reduce Task 两种， 均由 TaskTracker 启动。 HDFS 以固定大小的 block 为基本单位存储数据， 而对于 MapReduce 而言， 其处理单位是 split。split 是一个逻辑概念， 它只包含一些元数据信息， 比如数据起始位置、数据长度、数据所在节点等。它的划分方法完全由用户自己决定。 但需要注意的是，split 的多少决定了 Map Task 的数目 ，因为每个 split 会交由一个 Map Task 处理。

### 执行过程和生命周期
具体的执行过程分为map task的执行过程和reduce task的执行过程
- map task：Map Task 先将对应的 split 迭代解析成一个个 key/value 对，依次调用用户自定义的 map() 函数进行处理，最终将临时结果存放到本地磁盘上，其中临时数据被分成若干个 partition，每个 partition 将被一个Reduce Task 处理。
- Reduce Task 执行过程：该过程分为三个阶段
  1. 从远程节点上读取MapTask中间结果（称为“Shuffle 阶段”）；
  2. 按照key对key/value对进行排序（称为“ Sort 阶段”）；
  3. 依次读取<key, value list>，调用用户自定义的 reduce() 函数处理，并将最终结果存到 HDFS 上（称为“ Reduce 阶段”）。

>生命周期：

1. 作业提交与初始化：用户提交作业后， 首先由 JobClient 实例将作业相关信息， 比如将程序 jar 包、作业配置文件、 分片元信息文件等上传到分布式文件系统（ 一般为HDFS）上，其中，分片元信息文件记录了每个输入分片的逻辑位置信息。 然后 JobClient通过 RPC 通知 JobTracker。 JobTracker 收到新作业提交请求后， 由 作业调度模块对作业进行初始化。
2. 任务调度与监控：TaskTracker 周期性地通过 Heartbeat 向 JobTracker 汇报本节点的资源使用 情况， 一旦出 现空闲资源， JobTracker 会按照一定的策略选择一个合适的任务使用该空闲资源， 这由任务调度器完成。此外，JobTracker 跟踪作业的整个运行过程，并为作业的成功运行提供全方位的保障。 首先， 当 TaskTracker 或者Task 失败时， 转移计算任务 ； 其次， 当某个 Task 执行进度远落后于同一作业的其他 Task 时，为之启动一个相同 Task， 并选取计算快的 Task 结果作为最终结果。
3. 任务运行环境准备：运行环境准备包括 JVM 启动和资源隔离， 均由TaskTracker 实现。 TaskTracker 为每个 Task 启动一个独立的 JVM 以避免不同 Task 在运行过程中相互影响 ； 同时，TaskTracker 使用了操作系统进程实现资源隔离以防止 Task 滥用资源。
4. 任务执行：TaskTracker 为 Task 准备好运行环境后， 便会启动 Task。 在运行过程中， 每个 Task 的最新进度首先由 Task 通过 RPC 汇报给 TaskTracker， 再由 TaskTracker汇报给 JobTracker。

# YARN
## 基本概念，架构
YARN是一个资源管理、任务调度的框架，主要包含三大模块：ResourceManager（RM）、NodeManager（NM）、ApplicationMaster（AM）。其中，ResourceManager负责所有资源的监控、分配和管理；ApplicationMaster负责每一个具体应用程序的调度和协调；NodeManager负责每一个节点的维护。对于所有的applications，RM拥有绝对的控制权和对资源的分配权。而每个AM则会和RM协商资源，同时和NodeManager通信来执行和监控task。几个模块之间的关系如图所示。
[在这里插入图片描述](https://img-blog.csdnimg.cn/66a6cf585a4140ceae5ced31264d657e.png)

> ResourceManager
1. ResourceManager负责整个集群的资源管理和分配，是一个全局的资源管理系统。
2. NodeManager以心跳的方式向ResourceManager汇报资源使用情况（目前主要是CPU和内存的使用情况）。RM只接受NM的资源回报信息，对于具体的资源处理则交给NodeManager自己处理。
3. YARN Scheduler根据application的请求为其分配资源，不负责application job的监控、追踪、运行状态反馈、启动等工作。

> NodeManager
1. NodeManager是每个节点上的资源和任务管理器，它是管理这台机器的代理，负责该节点程序的运行，以及该节点资源的管理和监控。YARN集群每个节点都运行一个NodeManager。
2. NodeManager定时向ResourceManager汇报本节点资源（CPU、内存）的使用情况和Container的运行状态。当ResourceManager宕机时NodeManager自动连接RM备用节点。
3. NodeManager接收并处理来自ApplicationMaster的Container启动、停止等各种请求。

> ApplicationMaster
用户提交的每个应用程序均包含一个ApplicationMaster，它可以运行在ResourceManager以外的机器上。
1. 负责与RM调度器协商以获取资源（用Container表示）。
2. 将得到的任务进一步分配给内部的任务(资源的二次分配)。
3. 与NodeManager通信以启动/停止任务。
4. 监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。
5. 当前YARN自带了两个ApplicationMaster实现，一个是用于演示AM编写方法的实例程序DistributedShell，它可以申请一定数目的Container以并行运行一个Shell命令或者Shell脚本；另一个是运行MapReduce应用程序的AM—MRAppMaster。


注：ResourceManager只负责监控ApplicationMaster，并在ApplicationMaster运行失败时候启动它。ResourceManager不负责ApplicationMaster内部任务的容错，任务的容错由ApplicationMaster完成。

## 运行流程

![在这里插入图片描述](https://img-blog.csdnimg.cn/61154a140eb643c885f5e0496d7bf3e2.png)

1. client向RM提交应用程序，其中包括启动该应用的ApplicationMaster的必须信息，例如ApplicationMaster程序、启动ApplicationMaster的命令、用户程序等。
2. ResourceManager启动一个container用于运行ApplicationMaster。
3. 启动中的ApplicationMaster向ResourceManager注册自己，启动成功后与RM保持心跳。
4. ApplicationMaster向ResourceManager发送请求，申请相应数目的container。
5. ResourceManager返回ApplicationMaster的申请的containers信息。申请成功的container，由ApplicationMaster进行初始化。container的启动信息初始化后，AM与对应的NodeManager通信，要求NM启动container。AM与NM保持心跳，从而对NM上运行的任务进行监控和管理。
6. container运行期间，ApplicationMaster对container进行监控。container通过RPC协议向对应的AM汇报自己的进度和状态等信息。
7. 应用运行期间，client直接与AM通信获取应用的状态、进度更新等信息。
8. 应用运行结束后，ApplicationMaster向ResourceManager注销自己，并允许属于它的container被收回。

# Hbase
## 概念
base是分布式、面向列的开源数据库（其实准确的说是面向列族）
。HDFS为Hbase提供可靠的底层数据存储服务，MapReduce为Hbase提供高性能的计算能力，Zookeeper为Hbase提供稳定服务和Failover机制，因此我们说Hbase是一个通过大量廉价的机器解决海量数据的高速存储和读取的分布式数据库解决方案。

## 列存储
Hbase是根据Column Family列族来存储数据的。列族下面可以有非常多的列，列族在创建表的时候就必须指定。官方推荐的是列族最好小于或者等于3。我们使用的场景一般是1个列族。

- Rowkey ： Hbase只支持3中查询方式：基于Rowkey（类似于主键）的单行查询，基于Rowkey的范围扫描，全表扫描。
- Region：Region的概念和关系型数据库的分区或者分片差不多。Hbase会将一个大表的数据基于Rowkey的不同范围分配到不通的Region中，每个Region负责一定范围的数据访问和存储。这样即使是一张巨大的表，由于被切割到不通的region，访问起来的时延也很低。
- TimeStamp多版本：TimeStamp是实现Hbase多版本的关键。在Hbase中使用不同的timestame来标识相同rowkey行对应的不通版本的数据。在写入数据的时候，如果用户没有指定对应的timestamp，Hbase会自动添加一个timestamp，timestamp和服务器时间保持一致。在Hbase中，相同rowkey的数据按照timestamp倒序排列。默认查询的是最新的版本，用户可同指定timestamp的值来读取旧版本的数据。

## 存储架构
![在这里插入图片描述](https://img-blog.csdnimg.cn/e1f3501580d84cbe971a3b8f2b2a521f.png)

**Region** 是 HBase 中分布式存储和负载均衡的最小单元，不同的 Region 分布到不同的 RegionServer 上，如图 Table1 、 Table2 中均有多个 Region ，这些 Region 分布在不同的 **RegionServer** 中。

Region 虽然是分布式分布式存储的最小单元，但并不是存储的最小单元， **Store** 是存储的最小单元。Region 由一个或者多个 Store 组成，每个 Store 会保存一个 Column Family ；每个 Store 又由一个 **MemStore** 或 0 至多个 Hfile 组成；**MemStore 存储在内存中， HFile 存储在 HDFS 中。**

Hbase 在数据存储的过程当中，涉及到的物理对象分为如下：

- HMaster: 负责 DDL 创建或删除 tables ，同一时间只能有一个 active 状态的 master 存在。
- Zookeeper: 判定 HMaster 的状态，记录 Meta Table 的具体位置；
- Region: 一张 BigTable 的一个分片（ Shard ），记录着 key 的开始和结束；
- WAL: 预写日志，持久化且顺序存储，一个 RegionServer 维护一套 WAL ；
- RegionServer: RegionServer 中维护多个 region ， region 里包含 MemStore 以及多个 HFiles ；
- MemStore: 对应一个 BigTable 的 Column Family ，存在于文件缓存中，拥有文件句柄；
- BlockCache: 读缓存，存于内存；(Row-Key – > row) ；
- HFiles: 从 MemStore Flush 出来的文件，本身是持久化的，存储于 HDFS 的 DataNode 之中，每次 Flush 生成一个新的 HFile 文件，文件包含有序的键值对序列。

## 读写原理
![在这里插入图片描述](https://img-blog.csdnimg.cn/2dbe4469ac9e4ba6b3cc726ac8e09837.png)

>数据写入流程（如左图）：

1. 客户端首先从 Zookeeper 找到 meta 表的 region 位置，然后读取 meta 表中的数据， meta 表中存储了用户表的 region 信息。
2. 根据 namespace 、表名和 Row-Key 信息。找到写入数据对应的 region 信息
3. 找到这个 region 对应的 regionServer ，然后发送请求。
4. 把数据分别写到 HLog （ write ahead log ）和 memstore 各一份。
5. memstore 达到阈值后把数据刷到磁盘，生成 storeFile 文件。
6. 删除 HLog 中的历史数据。

>数据读出流程（如右图）：

1. 客户端首先与 Zookeeper 进行连接；从 Zookeeper 找到 meta 表的 region 位置，即 meta 表的数据存储在某一 HRegionServer 上；客户端与此 HRegionServer 建立连接，然后读取 meta 表中的数据；meta 表中存储了所有用户表的 region 信息，我们可以通过 scan 'hbase:meta' 来查看 meta 表信息。
2. 根据要查询的 namespace 、表名和 Row-Key 信息。找到数据对应的 region 信息。
3. 找到这个 region 对应的 regionServer 发送请求，并找到相应 region 。
4. 先从 memstore 查找数据，如果没有，再从 BlockCache 上读取。
5. 如果 BlockCache 中也没有找到，再到 StoreFile 上进行读取，从 storeFile 中读取到数据之后，不是直接把结果数据返回给客户端，而是把数据先写入到 BlockCache 中，目的是为了加快后续的查询；然后在返回结果给客户端。

## 存储引擎LSM-Tree
首先需要确定的是 Hbase 的存储引擎是 LSM-Tree。

我们知道 LSM-Tree 相比较 B+Tree 而言，最大的特点就是在于**通过牺牲部分读性能，利用分层合并的思想，将小树合并为大树，将无序数据合并为有序数据，然后统一刷入磁盘**，从而大大提高了写的性能。

那么 HBase 套用到 LSM 中， Memstore 就是 LSM 当中的 Memtable ，也就是 C0 层的小树写入， HFiles 就是 LSM 当中的 SSTables ，也就是 Cn 层的合并之后的树的顺序写入。

除此之外 Hbase 在实现 Hbase 的时候，其实还是有自己独到的地方：

- Minor vs Major Compaction ：Minor Compaction ，根据配置策略，自动检查小文件，合并到大文件，从而减少碎片文件，然而并不会立马删除掉旧 HFile 文件；Major Compaction ，每个 CF 中，不管有多少个 HFiles 文件，最终都是将 HFiles 合并到一个大的 HFile 中，并且把所有的旧 HFile 文件删除，即 CF 与 HFile 最终变成一一对应的关系。
- BlockCache ：除了 MemStore （也就是 MemTable ） 以外， HBase 还提供了另一种缓存结构， BlockCache 。BlockCache 本质上是将热数据放到内存里维护起来，避免 Disk I/O ，当然即使 BlockCache 找不到数据还是可以去 MemStore 中找的，只有两边都不存在数据的时候，才会读内存里的 HFile 索引寻址到硬盘，进行一次 I/O 操作。HBase 将 BucketCache 和 LRUBlockCache 搭配使用，称之为 CombinedBlockCache 。系统在 LRUBlockCache 中主要存储 Index Block ，而将 Data Block 存储在 BucketCache 中。因此一次随机读需要首先在 LRUBlockCache 中查到对应的 Index Block ，然后再到 BucketCache 查找对应数据块。
- HFile ：HFile 的数据结构也是 Hbase 的重要改进之处。
![在这里插入图片描述](https://img-blog.csdnimg.cn/4b02b2a023804c4a9d7cc33f3a9ce061.png)
主要包含四个部分：数据块、头信息、索引信息、地址信息。索引就是 HFile 内置的一个 B+ 树索引，当 RegionServer 启动后并且 HFile 被打开的时候，这个索引会被加载到 Block Cache 即内存里；KeyValues 存储在增长中的队列中的数据块里，数据块可以指定大小，默认 64k ，数据块越大，顺序检索能力越强；数据块越小，随机读写能力越强，需要权衡。