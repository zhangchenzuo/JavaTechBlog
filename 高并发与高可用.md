- [消息队列](#消息队列)
  - [几种消息队列的性能对比](#几种消息队列的性能对比)
  - [RocketMQ的可靠性](#rocketmq的可靠性)
    - [如何保证不发生消息丢失](#如何保证不发生消息丢失)
      - [如何保证producer往队列发消息时候不丢失](#如何保证producer往队列发消息时候不丢失)
    - [顺序消息](#顺序消息)
    - [消息重复](#消息重复)
  - [事务型消息](#事务型消息)
  - [RocketMQ的消息存储](#rocketmq的消息存储)
  - [消息订阅](#消息订阅)
  - [原理实现](#原理实现)
    - [消息持久化](#消息持久化)
- [反向代理（DNS），负载均衡，内容分发（CDN）](#反向代理dns负载均衡内容分发cdn)
  - [Nginx](#nginx)
    - [什么是Nginx](#什么是nginx)
    - [为什么可以高性能](#为什么可以高性能)
      - [epoll](#epoll)
      - [master-worker进程模型](#master-worker进程模型)
      - [协程模型](#协程模型)
- [高可用策略](#高可用策略)
  - [令牌桶](#令牌桶)
    - [RateLimiter限流实现](#ratelimiter限流实现)
- [分布式](#分布式)
  - [CAP与BASE](#cap与base)
    - [CAP](#cap)
    - [BASE](#base)
  - [一致性算法（RAFT）](#一致性算法raft)
  - [一致性哈希](#一致性哈希)
  - [两阶段提交和三阶段提交](#两阶段提交和三阶段提交)
    - [两阶段提交](#两阶段提交)
    - [三阶段提交](#三阶段提交)
    - [优缺点对比](#优缺点对比)
- [服务器CPU接近100%,如何去排查](#服务器cpu接近100如何去排查)
- [网站架构](#网站架构)
  - [Nginx层面](#nginx层面)
    - [负载均衡算法](#负载均衡算法)
    - [失败重试、长连接](#失败重试长连接)
  - [隔离](#隔离)
  - [限流](#限流)
    - [限流算法](#限流算法)
    - [应用级限流](#应用级限流)
    - [分布式限流](#分布式限流)
    - [节流](#节流)
  - [回滚](#回滚)
  - [缓存](#缓存)
    - [java缓存类型](#java缓存类型)
    - [缓存与数据库一致性问题](#缓存与数据库一致性问题)
    - [分布式缓存redis-cluster](#分布式缓存redis-cluster)
- [降低数据库的压力策略](#降低数据库的压力策略)
- [通信协议](#通信协议)
  - [远程过程调用协议(RPC)](#远程过程调用协议rpc)
  - [RESTful接口](#restful接口)
  


# 消息队列

消息队列在分布式系统中主要是为了解耦和削峰。

[参考文献-消息队列设计精要](https://tech.meituan.com/2016/07/01/mq-design.html)
[参考文献-RocketMq的问题](https://www.cnblogs.com/xuwc/p/9034352.html)

生产者（客户端）发送消息到消息队列中去，接受者（服务端）处理消息，需要消费的系统直接去消息队列取消息进行消费即可而不需要和其他系统有耦合，提高了系统的扩展性。

消息队列使利用发布-订阅模式工作，消息发送者（生产者）发布消息，一个或多个消息接受者（消费者）订阅消息。**我们只需要关心消息是否送达了队列，至于谁希望订阅，是下游的事情**。 从上图可以看到消息发送者（生产者）和消息接受者（消费者）之间没有直接耦合，消息发送者将消息发送至分布式消息队列即结束对消息的处理，消息接受者从分布式消息队列获取该消息后进行后续处理，并不需要知道该消息从何而来。对新增业务，只要对该类消息感兴趣，即可订阅该消息，对原有系统和业务没有任何影响，从而实现网站业务的可扩展性设计。


另外为了避免消息队列服务器宕机造成消息丢失，会将成功发送到消息队列的消息存储在消息生产者服务器上，等消息真正被消费者服务器处理后才删除消息。在消息队列服务器宕机后，生产者服务器会选择分布式消息队列服务器集群中的其他服务器发布消息。

基本结构包括了，**生产者和消费者，之间的桥梁是Topic，一个topic下可以有很多个Queue。**

**Broker**：消息的中转者，负责存储和转发消息。可以理解为消息队列服务器，提供了消息的接收、存储、拉取和转发服务。broker是RocketMQ的核心，它不不能挂的，所以需要保证broker的高可用。

## 几种消息队列的性能对比
![在这里插入图片描述](https://img-blog.csdnimg.cn/f1620c04ce8c421e94df9c507e341873.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pjejU1NjY3MTk=,size_16,color_FFFFFF,t_70)

## RocketMQ的可靠性
### 如何保证不发生消息丢失

消息丢失可以分为三种：
1. 生产者将数据发送到 MQ 的时候，可能数据就在半路给搞丢了，因为网络问题啥的，都有可能。此时可以选择事务功能，MQ收到消息给服务端一个回执，否则重试发送，再失败就回滚。（注意重复消费）
2. MQ自己丢失消息：开启持久化。
3. 消费端丢失：客户端从Broker拉取消息后，执行用户的消费业务逻辑，成功后，才会给Broker发送消费确认响应。如果Broker没有收到消费确认响应，下次拉消息的时候还会返回同一条消息，确认消息不会在网络传输过程中丢失，也不会因为客户端在执行消费逻辑中出错导致丢失。



- 从producer角度，消息发送方式可以同步也可以异步，但为了减小丢失消息的可能性尽量采用同步的发送方式，同步等待发送结果，利用同步发送+重试机制+多个master节点，尽可能减小消息丢失的可能性。
- 在broker端，消息丢失的可能性主要在于刷盘策略和同步机制。
- 从Consumer端先消费，消费成功后再提交；Consumer先pull 消息到本地，消费完成后，才向服务器返回ack。

消息队列所目标解决的问题核心还是**最终一致性问题**。最终一致性，主要是用“记录”和“补偿”的方式。**在做所有的不确定的事情之前，先把事情记录下来，然后去做不确定的事情**，结果可能是：成功、失败或是不确定，“不确定”（例如超时等）可以等价为失败。

成功就可以把记录的东西清理掉了，对于失败和不确定，可以依靠定时任务等方式把所有失败的事情重新搞一遍，直到成功为止。 

回到刚才的例子，系统在A扣钱成功的情况下，把要给B“通知”这件事记录在库里（为了保证最高的可靠性可以把通知B系统加钱和扣钱成功这两件事维护在一个本地事务里），通知成功则删除这条记录，通知失败或不确定则依靠**定时任务补偿性地通知我们**，直到我们把状态更新成正确的为止。 

整个这个模型依然可以基于RPC来做，但可以抽象成一个统一的模型，基于消息队列来做一个“企业总线”。 具体来说，**本地事务维护业务变化和通知消息**，一起落地（失败则一起回滚），然后RPC到达broker，在broker成功落地后，RPC返回成功，本地消息可以删除。**否则本地消息一直靠定时任务轮询不断重发**，这样就保证了消息可靠落地broker。

 broker往consumer发送消息的过程类似，一直发送消息，直到consumer发送消费成功确认。 我们先不理会重复消息的问题，通过两次消息落地加补偿，下游是一定可以收到消息的。然后依赖状态机版本号等方式做判重，更新自己的业务，就实现了最终一致性。

 #### 如何保证producer往队列发消息时候不丢失
redis 往MQ发送消息的时候，可能无法发送到mq -> 等不到回执 回滚。
但是这里存在一些极端的情况，也就是实际上发送了订单，但是回执mq发送不到redis这边。 导致实际上redis回滚了认为没下单，但是mq却开始下单了。

解决方法之一：可能几次生产者这边都没有收到回复，可能会去尝试去消费者那边查查，比如db，里面去看看是不是被消费了
 
 **其实所有的高可用，是依赖于RPC和存储的高可用来做的**。先来看RPC的高可用，美团的基于MTThrift的RPC框架，阿里的Dubbo等，其本身就具有服务自动发现，负载均衡等功能。而**消息队列的高可用，只要保证broker接受消息和确认消息的接口是幂等的，并且consumer的几台机器处理消息是幂等的，这样就把消息队列的可用性，转交给RPC框架来处理了**。

 那么怎么保证幂等呢？最简单的方式莫过于共享存储。broker多机器共享一个DB或者一个分布式文件/kv系统，则处理消息自然是幂等的。就算有单点故障，其他节点可以立刻顶上。 

### 顺序消息
其实RocketMq的设计思路上并不是要去解决很强的消息一致性，一般消息是通过轮询所有队列来发送的（负载均衡策略），**顺序消息可以根据业务，比如说订单号相同的消息发送到同一个队列。**

从源码角度分析RocketMQ怎么实现**发送**顺序消息。**这是在Producer端实现.**

一般消息是通过轮询所有队列来发送的（负载均衡策略），顺序消息可以根据业务，比如说**订单号相同的消息发送到同一个队列queue**。下面的示例中，OrderId相同的消息，会发送到同一个队列：
```java
// RocketMQ默认提供了两种MessageQueueSelector实现：随机/Hash
SendResult sendResult = producer.send(msg, new MessageQueueSelector() {
    @Override
    public MessageQueue select(List<MessageQueue> mqs, Message msg, Object arg) {
        Integer id = (Integer) arg;
        int index = id % mqs.size();
        return mqs.get(index);
    }
}, orderId);
```
在获取到路由信息以后，会根据MessageQueueSelector实现的算法来选择一个队列，同一个OrderId获取到的队列是同一个队列。

### 消息重复
我们一般要求at least once消息在传递时，至少会被送达一次。也就是说，不允许丢消息，但是允许有少量重复消息出现

> 1、消费端处理消息的业务逻辑保持幂等性 也就是重复处理不会变化
>
>2、保证每条消息都有唯一编号且保证消息处理成功与去重表的日志同时出现

我们可以看到第1条的解决方式，**很明显应该在消费端实现，不属于消息系统要实现的功能**。第2条可以消息系统实现，也可以业务端实现。正常情况下出现重复消息的概率不一定大，且由消息系统实现的话，肯定会对消息系统的吞吐量和高可用有影响，**所以最好还是由业务端自己处理消息重复的问题，这也是RocketMQ不解决消息重复的问题的原因。**

## 事务型消息

![在这里插入图片描述](https://img-blog.csdnimg.cn/171a697ad53b4867a924d862857164a0.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemN6NTU2NjcxOQ==,size_18,color_FFFFFF,t_70,g_se,x_16)

可以先向mq中提交一个“半消息”，然后去事务性的执行这个消息，mq会监听这个事务消息的执行状态，比如有没有抛出异常。如果抛出异常，就取消这个消息的发送。如果正常完成了这个任务，就将消息标记为`COMMIT`，然后真正的发送出去。

这里需要有一个“反查”函数，去检查这个事务是不是完成了，否则会长久的处于`UNKNOWN`状态。


>RocketMQ是基于两阶段提交来实现的事务，把这些事务消息暂存在一个特殊的queue中，待事务提交后再移动到业务队列中。最后，RocketMQ的事务适用于解决本地事务和发消息的数据一致性问题。

[参考资料 - RocketMQ支持事务消息机制](https://www.jianshu.com/p/cc5c10221aa1)

## RocketMQ的消息存储
消息的存储是由consume queue和commit log配合完成的。

consume queue是消息的逻辑队列，相当于字典的目录，用来指定消息在物理文件commit log上的位置。根据topic和queueId来组织文件，图中TopicA有两个队列0,1，那么TopicA和QueueId=0组成一个ConsumeQueue，TopicA和QueueId=1组成另一个ConsumeQueue。

按照消费端的GroupName来分组**重试队列**，**如果消费端消费失败，消息将被发往重试队列**中，比如图中的%RETRY%ConsumerGroupA。

按照消费端的GroupName来分组**死信队列**，**如果消费端消费失败，并重试指定次数后，仍然失败，则发往死信队列**，比如图中的%DLQ%ConsumerGroup

CommitLog：消息存放的物理文件，每台broker上的commitlog被本机所有的queue共享，不做任何区分。
## 消息订阅
有两种模式，Push和Pull，推拉两种方法。Push模式，即MQServer主动向消费端推送；另外一种是Pull模式，即消费端在需要时，主动到MQServer拉取。push就是长轮询，broker有消息就去响应。对于pull模式。如果broker在收到Pull请求时，消息队列里没有数据，broker端会阻塞请求直到有数据传递或超时才返回。

**一般情况下，topic的数量大于消费者**


## 原理实现
### 消息持久化
mq自身保证消息可以持久化的策略。

文件系统存储，常见的比如kafka、RocketMQ、RabbitMQ都是采用**消息刷盘**到所部署的机器上的文件系统来做持久化，这种方案适合对于有高吞吐量要求的消息中间件，因为消息刷盘是一种高效率，高可靠、高性能的持久化方式，除非磁盘出现故障，否则一般是不会出现无法持久化的问题。

>消息的存储结构:

RocketMQ就是采用文件系统的方式来存储消息，消**息的存储是由ConsumeQueue和CommitLog配合完成的**。

**CommitLog是消息真正的物理存储文件。ConsumeQueue是消息的逻辑队列，有点类似于数据库的索引文件，里面存储的是指向CommitLog文件中消息存储的地址。**

每个Topic下的每个Message Queue都会对应一个ConsumeQueue文件，文件的地址是：\${store_home}/consumequeue/${topicNmae}/\${queueId}/\${filename}, 默认路径: /root/store在rocketMQ的文件存储目录下，可以看到这样一个结构的的而文件。

>消息发送到消息接收的整体流程:

1. **producer将消息发送到Broker后，Broker会采用同步或者异步的方式把消息写入到CommitLog**。RocketMQ所有的消息都会存放在CommitLog中，为了保证消息存储不发生混乱，对CommitLog写之前会加锁，同时也可以使得消息能够被顺序写入到CommitLog，只要消息被持久化到磁盘文件CommitLog，那么就可以保证Producer发送的消息不会丢失。
2. **commitLog持久化后，会把里面的消息发送到对应的Consume Queue上**，Consume Queue相当于kafka中的partition，是一个逻辑队列，存储了这个Queue在CommiLog中的起始offset，log大小和MessageTag的hashCode。
3. **当消费者进行消息消费时，会先读取consumerQueue** , 逻辑消费队列ConsumeQueue保存了指定Topic下的队列消息在CommitLog中的起始物理偏移量Offset，消息大小、和消息Tag的HashCode值
4. 直接从consumequeue中读取消息是没有数据的，真正的消息主体在commitlog中，所以还需要**从commitlog中读取消息**


# 反向代理（DNS），负载均衡，内容分发（CDN）
这三个内容之所以放在一起描述，主要是因为这部分内容往往都是通过一些中间件帮助我们完成。比如我们这里经常使用到的Nginx，可以实现缓存静态资源（类似CDN），反向代理+负载均衡，将请求分散发送到后端服务器。
## Nginx
### 什么是Nginx
nginx是一款轻量级的Web服务器/反向代理服务器及电子邮件（IMAP/POP3）代理服务器。

nginx的特点是**占有内存少，并发能力强**。

可以支持高并发连接（2-3w），内存消耗小，配置简单。可以热部署，支持事件驱动。

作用：
1. 可以作为一个Http服务器，存储静态文件
2. 作为反向代理服务器。主要用于服务器集群分布式部署的情况下，反向代理隐藏了服务器的信息！
3. 支持负载均衡，可以采用轮询的方法，或者ip-hash保证固定ip地址的客户端总会访问到同一个后端服务器，这也在一定程度上解决了集群部署环境下session共享的问题。

### 为什么可以高性能
#### epoll
首先需要理解基本的事件处理模型，select，poll，epoll。

select的特点在于可以监视多个文件描述符的数组。**实现非阻塞的IO**，缺点在于存在最大的数量。

**poll的区别就是没有了最大的数量限制**。另外，select()和poll()将就绪的文件描述符告诉进程后，如果进程没有对其进行IO操作，那么下次调用select()和poll()的时候将再次报告这些文件描述符，所以它们一般不会丢失就绪的消息，这种方式称为水平触发（Level Triggered）。

nginx实现高性能的一个原因之一就是采用了epoll，e**poll同样只告知那些就绪的文件描述符，而且当我们调用epoll_wait()获得就绪文件描述符时，返回的不是实际的描述符，而是一个代表就绪描述符数量的值**，你只需要去epoll指定的一个数组中依次取得相应数量的文件描述符即可，这里也使用了内存映射技术，这样便彻底省掉了这些**文件描述符在系统调用时复制的开销**。

另一个改进在于epoll采用**基于事件的就绪通知方式**，epoll事先通过epoll_ctl()来注册一个文件描述符，一旦基于某个文件描述符就绪时，内核会采用类似callback的**回调机制**，迅速激活这个文件描述符，当进程调用epoll_wait()时便得到通知。在select/poll中，进程只有在调用一定的方法后，内核才对所有监视的文件描述符进行**扫描**。

另外支持**边缘触发**，**只告诉进程哪些文件描述符刚刚变为就绪状态，它只说一遍，如果我们没有采取行动，那么它将不会再次告知**。

打个简单的比方，我们都有订票的经验，当我们委托酒店订票时，接待员会先把我们的电话号码和相关信息等记下来（注册事件），挂断电话后接待员在操作期间我们就可以去做其他事了（非阻塞），当接待员把手续搞好后会主动打电话给我们通知我们票订好了（回调）。

#### master-worker进程模型
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210314200651648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pjejU1NjY3MTk=,size_16,color_FFFFFF,t_70)


nginx是多进程的，一个master进程，多个worker进程。**master进程管理worker进程。实际的请求都是由worker进程抢占的方法执行的**.

master 进程接受管理员的信号量（如 Nginx -s reload, -s stop）来管理 worker 进程，**master 本身并不接收 client 的请求，主要由 worker 进程来接收请求**。不同于 apache 的每个请求会占用一个线程，且是同步IO，**Nginx 是异步非阻塞的**，每个 worker 可以同时处理的请求数只受限于内存大小。master负责建立listen的socket，fork出worker进程，因此**worker进程之间都是独立的**。

worker 既然是互相独立的进程，就需要考虑其共享数据的问题， OpenResty 提供了一种高效的数据结构: **shared dict** ，可以实现在 worker 间共享数据，shared dict 对外提供了 20 多个 Lua API，都是原子操作的，避免了高并发下的竞争问题。

在进行**热切换**的时候，会新建mater进程执行，并且慢慢kill掉之前的进程。

#### 协程模型

协程是一种用户态的轻量级线程，协程的调度完全由用户控制。**协程拥有自己的寄存器上下文和栈**。**协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈**，直接操作栈则基本**没有内核切换**的开销，可以**不加锁的访问全局变量**，所以上下文的切换非常快。

nginx的每一个worker进程都是在epoll或者kqueue这种事件模型上，封装成一个协程。**完全不会阻塞worker进程，都是借助协程完成的**。**如果某个协程完成执行需要等待，会放弃自身**，然后把socket传个worker管理，在worker被epoll通知唤醒以后，新建一个协程去执行任务。因此具体任务的执行其实由协程完成的。worker进程将**将每个用户的请求对应到线程中的某一个协程**，然后在协程中，依靠 epoll 多路复用机制来完成同步调用开发。**降低上下文切换的消耗**。

- Worker 进程内的所有协程共享同一个 Lua 虚拟机；
- Nginx 的 Worker 进程是单线程模型的，其对 HTTP 请求的处理都是用协程完成的；
- Nginx 的协程遇到阻塞，依靠 epoll 模型解决

以但遇到阻塞，nginx会剥夺执行权力，交给其他的协程。因此不需要加锁。

# 高可用策略

高可用描述的是一个系统在大部分时间都是可用的，可以为我们提供服务的。高可用代表系统即使在发生硬件故障或者系统升级的时候，服务仍然是可用的。

1. 限流：限流是从用户访问压力的角度来考虑如何应对系统故障。

限流为了对服务端的接口接受请求的频率进行限制，防止服务挂掉。比如某一接口的请求限制为 100 个每秒, 对超过限制的请求放弃处理或者放到队列中等待处理。限流可以有效应对突发请求过多。

2. 降级：降级是从系统功能优先级的角度考虑如何应对系统故障。

服务降级指的是当服务器压力剧增的情况下，根据当前业务情况及流量对一些服务和页面有策略的降级，以此释放服务器资源以保证核心任务的正常运行。

3. 熔断：降级的目的在于应对系统自身的故障，而熔断的目的在于应对当前系统依赖的外部系统或者第三方系统的故障。

4. 集群：相同的服务部署多份，避免单点故障。

## 令牌桶

令牌桶和漏桶是两种常用的限流措施。

令牌桶：**处理突发流量**。采用guava的库，实现上是每秒钟有一些令牌，在发放完毕以后，查询下一秒的情况，如果下一秒没有发放完成。让请求的程序进行无法打断的sleep，将下一秒的令牌预支给他。相当于是一个定时器，每秒会新生成一定数量的令牌。

漏桶算法：**平滑网络流量**。每秒只能处理一定数量的请求。无法处理突发流量。

单机限流与集群限流：集群限流会需要一些中间件作为全局的计数器，可能导致一些性能瓶颈。因此为了简单的，我们采用单机限流的策略。这个策略在负载均衡做的比较好的时候效果很不错。

接口维度是限制某个接口的流量，总维度是限制所以接口的流量。

### RateLimiter限流实现
google.guava.RateLimiter就是令牌桶算法的一个实现类。设置了初始令牌数量为200。这个算法的之所以可以处理突发在于一个神奇的操作,**某次预先支付的令牌所需等待的时间让下一次请求来实际等待**。限制了QPS保护下游的负载。每秒钟只能处理200个请求。

比如crate(5)，但是我请求10个。会预支令牌，但是下一个请求就需要等待足2s的时间。并且在设置参数的地方可以有两个参数设置，一个是设置最大QPS，一个是每秒的生成的数量。

# 分布式
## CAP与BASE
### CAP
- C-Consistent 一致性，对于分布式系统，任意时刻访问任意实例得到的结果都是一样的。不保证返回时间。
- A-Availability 可用性，保证每个请求不管成功或者失败都有响应。不保证正确性。
- P-partition 分区容忍性。在任意分区网络故障的情况下系统仍能继续运行

CAP 原理就是——网络分区发生时，一致性和可用性两难全。
> CP -- 一致性和分区容错性。
  等待分区节点的响应可能会导致**延时错误**。如果你的业务需求需要**原子读写**，CP 是一个不错的选择。

>AP ─ 可用性与分区容错性。
响应节点上可用数据的最近版本可能并不是最新的。
如果业务需求允许最终一致性，或**当有外部故障时要求系统继续运行**，AP 是一个不错的选择。


### BASE
- BA基本可用（Basically Available）基本可用指分布式系统在出现故障时，系统允许损失部分可用性，即保证核心功能或者当前最重要功能可用。
- S软状态（Soft-state）
允许不同节点的副本之间存在暂时的不一致情况。
- E最终一致性（Eventually Consistent）
最终一致性要求系统中数据副本最终能够一致，而不需要
因此我们退而求其次，追求最终一致性BASE。通过异步消息队列等方法实现最终的一致性。

## 一致性算法（RAFT）

一致性就是数据保持一致，在分布式系统中，可以理解为多个节点中数据的值是一致的。

以比较了解的在redis的集群中使用到的**Raft**算法。

1. 故障检测：
集群中每个节点会定期向集群中其他节点发送PING，如果某个节点没有在规定时间内回复PONG。会被标记位疑似下线PFAIL。此时，集群中的节点会相互确认，如果在一**个集群中**，半数以上处理槽的**主节点**，都将某个主节点标注位疑似下线，这个主节点将会变为已下线FAIL，并向集群广播一条关于主节点的FAIL消息。

2. 故障转移：
当一个从节点发现自己的主节点出现了已下钱的标识，会开始进行故障转移：
。
>1. 集群的某个节点开始故障转移时，集群配置纪元`curruntepoch`都+1。
>2. 每个纪元中，每个主节点都只有一次投票机会。先到先得。
>3. 当从节点发现自己父节点先下线，向集群广播一下`CLUSTERMSG_TYPE_FAILOVER_AUTH_REQUEST`消息，要求收到的主节点投票。
>4. 可以进行投票的主节点会回复消息。
>5. 故障处理由延迟时间：<font color='red'> 500ms+random（0-500）+rank*1000ms</font>。固定延迟保证达成共识已经下线，random保证去同步性，rank是从节点之间通信，得到最新的offset的节点rank最低。
>6. 只要的票大于所有主节点的一半，称为成功。
## 一致性哈希

对于普通的hash算法，无论是增加还是减少节点，都有可能会改变映射关系，造成大量请求的 miss。那是否能避免大量的 miss 呢？答案也是肯定的，一致性哈希解决了节点增减造成大量 hash 重定位的问题。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210314210210336.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3pjejU1NjY3MTk=,size_16,color_FFFFFF,t_70)

1. 首先求出memcached服务器（节点）的哈希值，这里一般会引入“虚拟节点”的概念，将一个真实的服务器虚拟的映射成100个点。并将其配置到0～2^32的圆上。这样保证是均匀的分布。
2. 然后采用同样的方法求出存储数据的键的哈希值，并映射到相同的圆上。
3. 然后从数据映射到的位置开始顺时针查找，将数据保存到找到的第一个服务器上。如果超过2^32仍然找不到服务器，就会保存到第一台memcached服务器上。

## 两阶段提交和三阶段提交
### 两阶段提交
为了保持**分布式事务的原子性**，称为两阶段提交协议(2PC)。这种两阶段提交协议可以确保事务的更新可以提交给所有参与的资源，或者确保更新完全回滚到所有的资源外，资源恢复到事务开始前的状态。

两段提交协议把一个分布事务的所有分布管理分为两类：一个是协调者，所有其他的是参与者。协调者作出该事务是提交还是撤消的最后决定。所有的参与者负责管理相应的子事务的执行及各自局部数据库上执行写操作。

两段提交协议的内容是：

*  表决阶段：协调者向所有参与者发出“准备提交”信息。如果某个参与者准备提交，就回答就绪信息，否则回答撤消信息。**参与者在回答前应把有关信息写入自己的日志中。协调者在发出准备提交信息前也要把有关信息写入自己的日志中。**
如果在**规定时间内协调者收到所有参与者的就绪信息，就作出提交的决定，否则作出撤消的决定。**

* 执行阶段：协调者将有关决定的信息先写入日志，然后把这个决定发给所有的参与者。所有的参与者收到命令以后首先往自己的日志中**写入收到提交(或撤消)决定的信息，并向协调者发送应答信息，最后执行有关决定**。协调者收到所有参与者的应答信息后，一个事务执行到此结束。有关日志信息可以脱机保存。
  
采用两段提交协议以后，当系统发生故障时，各场地利用各自有关的日志信息(必要时加上后备副本)便可以执行恢复操作。

### 三阶段提交
3PC包括三段：决定段、准备提交段和执行段。

(1)第一段——决定段
* 协调者向所有参与者发出准备消息(prepare)(同2PC)。
* 若任一参与者回答中止Abort消息，则进入第三段(执行段)，协调者发出反转Rollback命令。
若所有参与者都回答Vote-Commit消息，则进入第二段(准备提交段)。

(2)第二段——准备提交段
* 由协调者发准备提交消息(Prepare to Commit)。
* 参与者收到该消息后写入运行记录(Log record)中，并回答确认消息ACK。
  
(3)第三段——执行段
* 协调者根据参与者回答的ACK/Abort消息，向参与者发Commit/Rollback命令。
* 参与者根据协调者的命令决定执行提交或回滚，完成事务的处理。 

### 优缺点对比

三阶段提交协议特点

* 优点：能避免阻塞状态，在三段提交协议中，如果协调者在第二段之后失效，不会产生像2PC协议中可能出现的事务阻塞现象

* 缺点：
需要更多的通讯次数，实现比较复杂。因此实际应用较少。大多数使用一致性提交协议的系统都采用二阶段提交协议。

# 服务器CPU接近100%,如何去排查
代码层面首先可以`top -c `查看当前的进程运行列表，然后按一下P可以按照CPU使用率进行排序。在找到最忙碌的进程以后`top -Hp 2609` 可以查看这个进程内的线程的信息。然后我们`jstack -l 2609 > ./2609.stack`打印出进程此时的快照，然后查看文件`cat 2609.stack |grep 'b26' -C 8`查看该线程在cpu占用100%时做了什么

# 网站架构
## Nginx层面
### 负载均衡算法
- 轮询算法（按权）：可以有效的避免热点堆积。
- ip_hash：根据用户的Ip进行负载均衡，同一个用户的请求到一个后端。
- Key值hash：对一个key进行一致性hash，可以最小化避免重新分配服务器。这个一般针对服务，根据uri进行均衡。

负载比较低时候，采用一致性哈希，热点请求降级为轮询的策略，或者如果请求有规律可以考虑使用带权重的一致性哈希。
### 失败重试、长连接
可以配置失败重试的请求，并且检测后端服务器的健康情况。支持TCP和HTTP心跳检测。

通过**配置keepalive参数可以配置和后端的长连接，但是对于客户端应该是短链接**。这样可以更多的用户接入。长连接记着启用HTTP1.1协议。

## 隔离
限定故障的传播范围，防止发生滚雪球效应。包括线程隔离（把请求分类，交给不同的线程池执行），进程隔离，读写隔离（读服务从redis集群获取数据，主集群出现问题，不影响从集群的读取）等。

## 限流
限制接口服务的最大可接入。一旦达到速率就拒绝。包括限制并发总数（线程池），限制瞬时并发量（nginx的limit_conn模块），限制时间窗口内的并发（RateLimiter），限制MQ的消费速度。

### 限流算法
限并发：在controller 的入口和结尾给令牌。比较粗暴

令牌桶：**处理突发流量**。采用guava的库，实现上是每秒钟有一些令牌，在发放完毕以后，查询下一秒的情况，如果下一秒没有发放完成。让请求的程序进行无法打断的sleep，将下一秒的令牌预支给他。相当于是一个定时器，每秒会新生成一定数量的令牌。

漏桶算法：**平滑网络流量**。每秒只能处理一定数量的请求。无法处理突发流量。

单机限流与集群限流：集群限流会需要一些中间件作为全局的计数器，可能导致一些性能瓶颈。因此为了简单的，我们采用单机限流的策略。这个策略在负载均衡做的比较好的时候效果很不错。

接口维度是限制某个接口的流量，总维度是限制所以接口的流量。

* RateLimiter限流实现

google.guava.RateLimiter就是令牌桶算法的一个实现类。设置了初始令牌数量为200。这个算法的之所以可以处理突发在于一个神奇的操作,**某次预先支付的令牌所需等待的时间让下一次请求来实际等待**。限制了QPS保护下游的负载。每秒钟只能处理200个请求。

比如crate(5)，但是我请求10个。会预支令牌，但是下一个请求就需要等待足2s的时间。

* 平滑预测限流
  
SmoothWarmingUp，平滑的启动系统。

### 应用级限流
可以使用guava cache实现，cache存储一个计数器，key为请求的函数+秒的粒度。过期时间设置为2s，保证能够记录下来一秒内的计数，这样最多会超越两倍的流量。然后我们通过当前的时间戳的秒作为key来计数和限流。 限流一般无法做到非常的精准。

### 分布式限流
核心是需要将限流服务原子化，解决方案可以是redis+lua或者Nginx+lua。思路是也是类似的采用2s的过期窗口。因为redis是单线程的所以是线程安全的。而nginx是使用shared-dict实现，需要使用lua-resty-lock保证原子性。

### 节流
特定的窗口内对重复相同的时间最多只处理一次，或者想限制多个连续相同时间最小执行时间间隔。

## 回滚
单机的事务回滚很容易，分布式回滚需要使用两阶段提交、三阶段提交。并且一般场景只需要最终的一致性即可。

如果JVM实例挂掉了，需要扫描事务日志，重新拉起。或者定期扫描优惠券和库存表，回滚失败的订单。核心是保持最终一致性。

## 缓存
1. 浏览器缓存，设置Cache-control等，适合对实时性不敏感的数据，比如商家评价，广告词等。
   
2. CDN缓存：将活动页面，图片等推到距离用户最近的CDN节点。包括推送机制，当内容变更以后主动推送到CDN边缘节点，和拉取机制，先访问边缘节点，如果没有回源到源服务节点。当然这里需要考虑URL的设计，不能有随机数，否则每次都会穿透CDN进行回源。（这也是一个好的防止缓存的策略）
   
3. 接入层缓存：
   1. Nginx层。可以进行URL重写，去除随机数；
   2. 一致性哈希：按照参数进行一致性hash，保证相同的数据落到一台服务器上。
   3. shared-dict，使用nginx+lua架构。使用lua-resty-lock非阻塞锁可以有效减少峰值的回源量。

衡量缓存性能的指标一般就是命中率。

### java缓存类型
- 堆缓存：使用java堆进行缓存。好处是速度快，不需要进行序列化。缺点是会给GC压力。一般可以使用软/弱引用，来存储对象。这样可以在空间不足时候被很好的回收。一般存储比较热门的数据Guava cache。

- 堆外缓存：好处是空间更大，减少GC。缺点是需要序列化反序列化，稍慢。MapDB
- 分布式缓存：解决单机容量和多台jvm的缓存一致性

堆内存存储最热的数据，相对热的存储到堆外，全量数据分布式缓存。

### 缓存与数据库一致性问题
[如何解决读写一致性](https://developer.aliyun.com/article/712285)
1. 如果先更新数据库再更新缓存：会产生脏写问题。
![](https://img-blog.csdnimg.cn/img_convert/0e24b8992ebab233d782b76df5e56104.png)
首先不可以采用先更新缓存再更新数据库，会发生异步的脏读。
2. 先删除缓存，再更新数据库：
 - 请求A进行写操作，删除缓存 
 - 请求B查询发现缓存不存在 
 - 请求B去数据库查询得到旧值 
 - 请求B将旧值写入缓存 
 - 请求A将新值写入数据库 上述情况就会导致不一致的情形出现。而且，如果不采用给缓存设置过期时间策略，该数据永远都是脏数据。 

解决的思路：延时双删策略，（1）先淘汰缓存 （2）再写数据库（这两步和原来一样）（3）休眠1秒，再次淘汰缓存这么做，可以将1秒内所造成的缓存脏数据，再次删除。 

3. 先更新数据库，再删除缓存（FaceBook）
  **写先写db**，然后在redis中更新；**读先读redis**，没有去db中更新到redis中。
 * 失效：应用程序先从cache取数据，没有得到，则从数据库中取数据，成功后，放到缓存中。
 * 命中：应用程序从cache中取数据，取到后返回。
 * 更新：先把数据存到数据库中，成功后，再让缓存失效。
这种策略的核心在于认为**数据库的读操作的速度远快于写操作的**。也就是在数据库写入新的数据以后，那些读请求早就结束了。

 **异步缓存写入：只更新缓存，不直接更新 DB，而是改为异步批量的方式来更新 DB。比如采用消息队列的方法。**
### 分布式缓存redis-cluster
基于槽指派的一种分布式缓存策略。


# 降低数据库的压力策略
1. **读写分离** : 读写分离主要是为了将数据库的读和写操作分不到不同的数据库节点上。**主服务器负责写，从服务器负责读**。另外，一主一从或者一主多从都可以。读写分离可以大幅提高读性能，小幅提高写的性能。因此，读写分离更适合单机并发读请求比较多的场景。

2. **分库分表** : 分库分表是为了解决由于库、表数据量过大，而导致数据库性能持续下降的问题。 
3. **负载均衡** : 负载均衡系统通常用于将任务比如用户请求处理分配到多个服务器处理以提高网站、应用或者数据库的性能和可靠性。

# 通信协议
## 远程过程调用协议(RPC)
在 RPC 中，客户端会去调用另一个地址空间（通常是一个远程服务器）里的方法。调用代码看起来就像是调用的是一个本地方法，客户端和服务器交互的具体过程被抽象。远程调用相对于本地调用一般较慢而且可靠性更差，因此区分两者是有帮助的。

**RPC 专注于暴露方法**。一般用于系统内的调用。

RPC方法示例：
```java
GET /someoperation?data=anId

POST /anotheroperation
{
  "data":"anId";
  "anotherdata": "another value"
}

```

## RESTful接口
是一种风格，使用了HTTP的协议去完成调用。更在意的是于数据之间打交道，**REST 关注于暴露数据**。它减少了客户端／服务端的耦合程度，经常用于公共 HTTP API 接口设计。因此使用的都是HTTP中规定的操作，GET、POST、PUT等。

![在这里插入图片描述](https://img-blog.csdnimg.cn/b571a78466f142c4a99553965e522d1f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAemN6NTU2NjcxOQ==,size_20,color_FFFFFF,t_70,g_se,x_16)